{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You'll generate plots of attention in order to see which parts of an image\n",
    "# our model focuses on during captioning\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scikit-learn includes many helpful utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import tqdm\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "把training跟validation data的file path存入img_names，把caption存入captions  \n",
    "testing data的file path存入test_img_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images:  120000\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "img_names = []\n",
    "captions = []\n",
    "with open(\"./words_captcha/spec_train_val.txt\") as f:\n",
    "    for line in f:\n",
    "        spec = line.strip().split()\n",
    "        img_names.append(f'./words_captcha/{spec[0]}.png')\n",
    "        captions.append('<start> ' + ' '.join(spec[1]) + ' <end>')\n",
    "\n",
    "# find all filenames in the directory\n",
    "all_img_path = glob.glob(f'./words_captcha/*.png')\n",
    "all_img_path = [\"./\"+ Path(path).as_posix() for path in all_img_path]\n",
    "test_img_names = set(all_img_path)-set(img_names)\n",
    "test_img_names = list(test_img_names)\n",
    "print(\"Number of training images: \", len(img_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess and tokenization \n",
    "這邊參考助教給的notebook，但是tokenizer不設max k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=\"<unk>\",filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'\n",
    "tokenizer.fit_on_texts(captions)\n",
    "train_seqs = tokenizer.texts_to_sequences(captions)\n",
    "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\n",
    "max_length = len(cap_vector[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split training, validating, and testing data \n",
    "使用train test split把data分成training跟validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 20000\n",
      "100000 20000\n"
     ]
    }
   ],
   "source": [
    "# Create training and validation sets using an 100-20 split\n",
    "img_names_train, img_names_val, cap_train, cap_val = train_test_split(img_names, cap_vector, test_size=20000, train_size=100000, random_state=0)\n",
    "print(len(img_names_train), len(img_names_val))\n",
    "print(len(cap_train), len(cap_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (160, 300)\n",
    "BATCH_SIZE = 100\n",
    "BUFFER_SIZE = 5000\n",
    "EPOCHS = 15\n",
    "EMBEDDING_DIM = 256\n",
    "UNITS = 512\n",
    "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
    "STEPS = len(img_names_train) // BATCH_SIZE\n",
    "# Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
    "# These two variables represent that vector shape                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
    "LEARNING_RATE = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_func(img_path, cap):\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.image.decode_png(img, channels=3)\n",
    "    img = tf.image.resize(img, IMAGE_SIZE)\n",
    "    # turn the value to range of [-1, 1]\n",
    "    img = img / 255.0 * 2 - 1\n",
    "    return img, cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = tf.data.Dataset.from_tensor_slices((img_names_train, cap_train))\\\n",
    "                               .map(map_func, num_parallel_calls=tf.data.experimental.AUTOTUNE)\\\n",
    "                               .shuffle(BUFFER_SIZE)\\\n",
    "                               .batch(BATCH_SIZE, drop_remainder=True)\\\n",
    "                               .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "dataset_valid = tf.data.Dataset.from_tensor_slices((img_names_val, cap_val))\\\n",
    "                               .map(map_func, num_parallel_calls=tf.data.experimental.AUTOTUNE)\\\n",
    "                               .batch(BATCH_SIZE, drop_remainder=True)\\\n",
    "                               .prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 2  7 11 13  3  0  0]\n",
      " [ 2  6  9 17 13  3  0]\n",
      " [ 2  9  8 19 18  9  3]\n",
      " [ 2  5  6 18  3  0  0]\n",
      " [ 2 24  5 25  3  0  0]\n",
      " [ 2  8 10  5  3  0  0]\n",
      " [ 2 15  8 14  3  0  0]\n",
      " [ 2  8 15  7 13  3  0]\n",
      " [ 2 18  8 10  4  3  0]\n",
      " [ 2 19  7 12  4  3  0]\n",
      " [ 2 14 15 17  3  0  0]\n",
      " [ 2 16  8  6 14  3  0]\n",
      " [ 2 15  5 16  3  0  0]\n",
      " [ 2 14 11  4  5 10  3]\n",
      " [ 2 15 10  8 28  4  3]\n",
      " [ 2  7 24 12  4 10  3]\n",
      " [ 2 16  5 27  3  0  0]\n",
      " [ 2 15 25 14  3  0  0]\n",
      " [ 2 19 20  5  3  0  0]\n",
      " [ 2 20  5 23  4 10  3]\n",
      " [ 2  6  7 14  3  0  0]\n",
      " [ 2 15  7  6  9  6  3]\n",
      " [ 2 14 17 20  4  3  0]\n",
      " [ 2  6  8 28  4  6  3]\n",
      " [ 2 21  7 27  3  0  0]\n",
      " [ 2 10  7 13  3  0  0]\n",
      " [ 2 24  4 13  3  0  0]\n",
      " [ 2 14 18  4 14 23  3]\n",
      " [ 2 17 15  7 12  3  0]\n",
      " [ 2 20  7  7  9  6  3]\n",
      " [ 2 15  7 15  4  3  0]\n",
      " [ 2  7 24 12  4 10  3]\n",
      " [ 2  5  6 15  3  0  0]\n",
      " [ 2  8 13  4  3  0  0]\n",
      " [ 2 12  5  9  7  3  0]\n",
      " [ 2 13  4 15  9  3  0]\n",
      " [ 2 11  7 19  7  3  0]\n",
      " [ 2  5 19  7  3  0  0]\n",
      " [ 2 10  7 11 11  3  0]\n",
      " [ 2 18  8  9  6  3  0]\n",
      " [ 2  6 24  8  6  6  3]\n",
      " [ 2 24  5 11 11  6  3]\n",
      " [ 2  9  8  4 13  3  0]\n",
      " [ 2 14 18  8 12  5  3]\n",
      " [ 2 10  5  8 12  3  0]\n",
      " [ 2 14 11  8 15  6  3]\n",
      " [ 2 13  4  5 12  3  0]\n",
      " [ 2 28 17 16  3  0  0]\n",
      " [ 2 15  7 11 11  3  0]\n",
      " [ 2 16  4  9  3  0  0]\n",
      " [ 2 10  7 17 12 13  3]\n",
      " [ 2  9 18  4 16  4  3]\n",
      " [ 2 20  5 22  3  0  0]\n",
      " [ 2 13 10 22  3  0  0]\n",
      " [ 2 19 17  8  3  0  0]\n",
      " [ 2 24 18  7  6  4  3]\n",
      " [ 2  6  9  5 19  4  3]\n",
      " [ 2 13  8  6 14  3  0]\n",
      " [ 2  6 11  8 13  4  3]\n",
      " [ 2 20 10  7  5 13  3]\n",
      " [ 2  6  7 14  3  0  0]\n",
      " [ 2  6 18  8 15  6  3]\n",
      " [ 2  9 18  4 16  4  3]\n",
      " [ 2 26  5 12  3  0  0]\n",
      " [ 2 17 12  5  3  0  0]\n",
      " [ 2  7 18  8  7  3  0]\n",
      " [ 2  9  8 16  4  3  0]\n",
      " [ 2  7  7  7  3  0  0]\n",
      " [ 2  9  5  6 23  6  3]\n",
      " [ 2 10  8 19 18  9  3]\n",
      " [ 2 26 17 13 22  3  0]\n",
      " [ 2 20  8 12 13  3  0]\n",
      " [ 2 21  5  9  3  0  0]\n",
      " [ 2 19 17  4  6  6  3]\n",
      " [ 2 15  4 10  3  0  0]\n",
      " [ 2 11  5 16 15  3  0]\n",
      " [ 2 23 12  4 24  3  0]\n",
      " [ 2  6 14  8  3  0  0]\n",
      " [ 2 13  8  4  9  3  0]\n",
      " [ 2 13 17  9 22  3  0]\n",
      " [ 2 19  4 11  3  0  0]\n",
      " [ 2  6  5 12  3  0  0]\n",
      " [ 2 14  5 10  3  0  0]\n",
      " [ 2  9  4 11 11  6  3]\n",
      " [ 2  5  6 18  3  0  0]\n",
      " [ 2  5 15  9  3  0  0]\n",
      " [ 2 20  8 20 11  4  3]\n",
      " [ 2  5 20  7 25  4  3]\n",
      " [ 2  9  4 16 15  3  0]\n",
      " [ 2 14 10 22  3  0  0]\n",
      " [ 2 24  5  6 18  3  0]\n",
      " [ 2 10  4  5 13  3  0]\n",
      " [ 2 11  4 21  9  3  0]\n",
      " [ 2 29 17  8  9  3  0]\n",
      " [ 2 13  8  5 11  3  0]\n",
      " [ 2  5 25  4  3  0  0]\n",
      " [ 2 24  7 10 12  3  0]\n",
      " [ 2  4 10  5  3  0  0]\n",
      " [ 2 26  4  9  6  3  0]\n",
      " [ 2 12  4 14  3  0  0]], shape=(100, 7), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(next(iter(dataset_train))[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction\n",
    "這裡我使用yolo的hidden state的output當作extract出來的feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_leaky_relu(inputs, filters, size, stride):\n",
    "    x = layers.Conv2D(filters, size, stride, padding=\"same\",\n",
    "                      kernel_initializer=tf.keras.initializers.TruncatedNormal())(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.1)(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_inputs = keras.Input(shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
    "x = conv_leaky_relu(img_inputs, 64, 7, 2)\n",
    "x = layers.MaxPool2D()(x)\n",
    "x = conv_leaky_relu(x, 192, 3, 1)\n",
    "x = layers.MaxPool2D()(x)\n",
    "x = conv_leaky_relu(x, 128, 1, 1)\n",
    "x = conv_leaky_relu(x, 256, 3, 1)\n",
    "x = conv_leaky_relu(x, 256, 1, 1)\n",
    "x = conv_leaky_relu(x, 512, 3, 1)\n",
    "x = layers.MaxPool2D()(x)\n",
    "x = conv_leaky_relu(x, 256, 1, 1)\n",
    "x = conv_leaky_relu(x, 512, 3, 1)\n",
    "x = conv_leaky_relu(x, 256, 1, 1)\n",
    "x = conv_leaky_relu(x, 512, 3, 1)\n",
    "x = conv_leaky_relu(x, 256, 1, 1)\n",
    "x = conv_leaky_relu(x, 512, 3, 1)\n",
    "x = conv_leaky_relu(x, 256, 1, 1)\n",
    "x = conv_leaky_relu(x, 512, 3, 1)\n",
    "x = conv_leaky_relu(x, 512, 1, 1)\n",
    "x = conv_leaky_relu(x, 1024, 3, 1)\n",
    "x = layers.MaxPool2D()(x)\n",
    "x = conv_leaky_relu(x, 512, 1, 1)\n",
    "x = conv_leaky_relu(x, 1024, 3, 1)\n",
    "x = conv_leaky_relu(x, 512, 1, 1)\n",
    "x = conv_leaky_relu(x, 1024, 3, 1)\n",
    "x = conv_leaky_relu(x, 1024, 3, 1)\n",
    "x = conv_leaky_relu(x, 1024, 3, 2)\n",
    "x = conv_leaky_relu(x, 1024, 3, 1)\n",
    "outputs = conv_leaky_relu(x, 1024, 3, 1)\n",
    "\n",
    "feature_extractor = keras.Model(inputs=img_inputs, outputs=outputs, name=\"YOLO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"YOLO\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 160, 300, 3)]     0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 80, 150, 64)       9472      \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 80, 150, 64)      256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 80, 150, 64)       0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 40, 75, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 40, 75, 192)       110784    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 40, 75, 192)      768       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 40, 75, 192)       0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 20, 37, 192)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 20, 37, 128)       24704     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 20, 37, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 20, 37, 128)       0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 20, 37, 256)       295168    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 20, 37, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 20, 37, 256)       0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 20, 37, 256)       65792     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 20, 37, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 20, 37, 256)       0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 20, 37, 512)       1180160   \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 20, 37, 512)      2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_5 (LeakyReLU)   (None, 20, 37, 512)       0         \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 10, 18, 512)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 10, 18, 256)       131328    \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 10, 18, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_6 (LeakyReLU)   (None, 10, 18, 256)       0         \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 10, 18, 512)       1180160   \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 10, 18, 512)      2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_7 (LeakyReLU)   (None, 10, 18, 512)       0         \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 10, 18, 256)       131328    \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 10, 18, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_8 (LeakyReLU)   (None, 10, 18, 256)       0         \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 10, 18, 512)       1180160   \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 10, 18, 512)      2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_9 (LeakyReLU)   (None, 10, 18, 512)       0         \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 10, 18, 256)       131328    \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 10, 18, 256)      1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_10 (LeakyReLU)  (None, 10, 18, 256)       0         \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 10, 18, 512)       1180160   \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 10, 18, 512)      2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_11 (LeakyReLU)  (None, 10, 18, 512)       0         \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 10, 18, 256)       131328    \n",
      "                                                                 \n",
      " batch_normalization_12 (Bat  (None, 10, 18, 256)      1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_12 (LeakyReLU)  (None, 10, 18, 256)       0         \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 10, 18, 512)       1180160   \n",
      "                                                                 \n",
      " batch_normalization_13 (Bat  (None, 10, 18, 512)      2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_13 (LeakyReLU)  (None, 10, 18, 512)       0         \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 10, 18, 512)       262656    \n",
      "                                                                 \n",
      " batch_normalization_14 (Bat  (None, 10, 18, 512)      2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_14 (LeakyReLU)  (None, 10, 18, 512)       0         \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 10, 18, 1024)      4719616   \n",
      "                                                                 \n",
      " batch_normalization_15 (Bat  (None, 10, 18, 1024)     4096      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_15 (LeakyReLU)  (None, 10, 18, 1024)      0         \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 5, 9, 1024)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_16 (Conv2D)          (None, 5, 9, 512)         524800    \n",
      "                                                                 \n",
      " batch_normalization_16 (Bat  (None, 5, 9, 512)        2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_16 (LeakyReLU)  (None, 5, 9, 512)         0         \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          (None, 5, 9, 1024)        4719616   \n",
      "                                                                 \n",
      " batch_normalization_17 (Bat  (None, 5, 9, 1024)       4096      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_17 (LeakyReLU)  (None, 5, 9, 1024)        0         \n",
      "                                                                 \n",
      " conv2d_18 (Conv2D)          (None, 5, 9, 512)         524800    \n",
      "                                                                 \n",
      " batch_normalization_18 (Bat  (None, 5, 9, 512)        2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_18 (LeakyReLU)  (None, 5, 9, 512)         0         \n",
      "                                                                 \n",
      " conv2d_19 (Conv2D)          (None, 5, 9, 1024)        4719616   \n",
      "                                                                 \n",
      " batch_normalization_19 (Bat  (None, 5, 9, 1024)       4096      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_19 (LeakyReLU)  (None, 5, 9, 1024)        0         \n",
      "                                                                 \n",
      " conv2d_20 (Conv2D)          (None, 5, 9, 1024)        9438208   \n",
      "                                                                 \n",
      " batch_normalization_20 (Bat  (None, 5, 9, 1024)       4096      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_20 (LeakyReLU)  (None, 5, 9, 1024)        0         \n",
      "                                                                 \n",
      " conv2d_21 (Conv2D)          (None, 3, 5, 1024)        9438208   \n",
      "                                                                 \n",
      " batch_normalization_21 (Bat  (None, 3, 5, 1024)       4096      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_21 (LeakyReLU)  (None, 3, 5, 1024)        0         \n",
      "                                                                 \n",
      " conv2d_22 (Conv2D)          (None, 3, 5, 1024)        9438208   \n",
      "                                                                 \n",
      " batch_normalization_22 (Bat  (None, 3, 5, 1024)       4096      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_22 (LeakyReLU)  (None, 3, 5, 1024)        0         \n",
      "                                                                 \n",
      " conv2d_23 (Conv2D)          (None, 3, 5, 1024)        9438208   \n",
      "                                                                 \n",
      " batch_normalization_23 (Bat  (None, 3, 5, 1024)       4096      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_23 (LeakyReLU)  (None, 3, 5, 1024)        0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 60,208,704\n",
      "Trainable params: 60,182,336\n",
      "Non-trainable params: 26,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "feature_extractor.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN encoder\n",
    "參考助教的notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        # x shape after passing through fc == (batch_size, 15, embedding_dim)\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN decoder\n",
    "參考助教的notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
    "\n",
    "        # hidden shape == (batch_size, hidden_size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "        # score shape == (batch_size, 64, hidden_size)\n",
    "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "\n",
    "        # attention_weights shape == (batch_size, 64, 1)\n",
    "        # you get 1 at the last axis because you are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "    def call(self, x, features, hidden):\n",
    "        # defining attention as a separate model\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # shape == (batch_size, max_length, hidden_size)\n",
    "        x = self.fc1(output)\n",
    "\n",
    "        # x shape == (batch_size * max_length, hidden_size)\n",
    "        x = tf.reshape(x, (-1, x.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size * max_length, vocab)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x, state, attention_weights\n",
    "\n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CNN_Encoder(EMBEDDING_DIM)\n",
    "decoder = RNN_Decoder(EMBEDDING_DIM, UNITS, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoints\n",
    "參考助教的notebook，不過要多存feature extractor，因為我不是用pre-trained的model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "ckpt = tf.train.Checkpoint(feature_extractor=feature_extractor,\n",
    "                           encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer = optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "# if ckpt_manager.latest_checkpoint:\n",
    "#     start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding this in a separate cell because if you run the training cell\n",
    "# many times, the loss_plot array will be reset\n",
    "loss_plot = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "參考助教的notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "    loss = 0\n",
    "\n",
    "    # initializing the hidden state for each batch\n",
    "    # because the captions are not related from image to image\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        features = feature_extractor(img_tensor, True)\n",
    "        features = tf.reshape(features, (features.shape[0], -1, features.shape[3]))\n",
    "        features = encoder(features)\n",
    "\n",
    "        for i in range(1, target.shape[1]):\n",
    "            # passing the features through the decoder\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "            \n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "    trainable_variables = feature_extractor.trainable_variables + encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "預測img的caption，然後把預測的caption中的start，end，padding都拿掉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(img_tensor):\n",
    "    batch_size = img_tensor.shape[0]\n",
    "    dec_input = tf.expand_dims(\n",
    "        [tokenizer.word_index['<start>']] * batch_size, 1)\n",
    "\n",
    "    features = feature_extractor(img_tensor)\n",
    "    features = tf.reshape(features, (features.shape[0], -1, features.shape[3]))\n",
    "    features = encoder(features)\n",
    "\n",
    "    hidden = decoder.reset_state(batch_size=batch_size)\n",
    "\n",
    "    result = tf.expand_dims([tokenizer.word_index['<start>']] * batch_size, 1)\n",
    "    for _ in range(max_length):\n",
    "        predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "        predicted_id = tf.argmax(predictions, axis=1).numpy()\n",
    "        dec_input = tf.expand_dims(predicted_id, 1)\n",
    "        result = tf.concat([result, predicted_id.reshape((batch_size, 1))], axis=1)\n",
    "    \n",
    "    actual_result = []\n",
    "    for r in result.numpy():\n",
    "        seq = \"\"\n",
    "        for s in r[1:]:\n",
    "            if s == tokenizer.word_index['<end>']:\n",
    "                break\n",
    "            seq += tokenizer.index_word[s]\n",
    "        actual_result.append(seq)\n",
    "        \n",
    "    return actual_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在每個epoch都做一次evaluation看validation accuracy多少  \n",
    "在evaluation要exact match才能算correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    \n",
    "    loss = 0\n",
    "    pbar = tqdm.tqdm(enumerate(dataset_train), total=STEPS)\n",
    "    for (batch, (img_tensor, target)) in pbar:\n",
    "        loss += train_step(img_tensor, target)\n",
    "        pbar.set_postfix({'loss': loss.numpy() / (batch + 1)})\n",
    "\n",
    "    # # storing the epoch end loss value to plot later\n",
    "    # loss_plot.append(loss / STEPS)\n",
    "    # ckpt_manager.save()\n",
    "    \n",
    "    # correct = 0\n",
    "        for img_tensor, target in dataset_valid:\n",
    "            pred_list = predict(img_tensor)\n",
    "            real_list = []\n",
    "            for r in target.numpy():\n",
    "                seq = \"\"\n",
    "                for s in r[1:]:\n",
    "                    if s == tokenizer.word_index['<end>']:\n",
    "                        break\n",
    "                    seq += tokenizer.index_word[s]\n",
    "                real_list.append(seq)\n",
    "                \n",
    "            for pred, real in zip(pred_list, real_list):\n",
    "                if pred == real:\n",
    "                    correct += 1\n",
    "    print(f'Validation accuracy: {correct/len(img_names_val):.2f}')\n",
    "    \n",
    "print ('Time taken for {} epoch {} sec\\n'.format(EPOCHS, time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "雖然loss一直有在降，但validation accuracy卻是會上下震盪  \n",
    "在testing我用最後一個epoch的model來predict，因為他的validation accuracy有達到0.94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x1d20862f7c0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restore the latest checkpoint and test\n",
    "ckpt.restore(ckpt_manager.latest_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_test(img_path):\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.image.decode_png(img, channels=3)\n",
    "    img = tf.image.resize(img, IMAGE_SIZE)\n",
    "    img = img / 255 * 2 - 1\n",
    "    return img, img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = tf.data.Dataset.from_tensor_slices((test_img_names))\\\n",
    "                              .map(map_test, num_parallel_calls=tf.data.experimental.AUTOTUNE)\\\n",
    "                              .batch(100)\\\n",
    "                              .prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:54<00:00,  3.70it/s]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "with open('./Lab12-2_113062556.txt', 'w') as fout:\n",
    "    pbar = tqdm.tqdm(enumerate(dataset_test), total = len(test_img_names) // 100)\n",
    "    for step, (img_tensor, img_path) in pbar:\n",
    "        pred_list = predict(img_tensor)\n",
    "        for path, pred in zip(img_path, pred_list):\n",
    "            path = path.numpy().decode('utf-8')\n",
    "            name = os.path.splitext(os.path.basename(path))[0]\n",
    "            fout.write(f'{name} {pred}\\n')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
